DATA:
  dataname: #your dataset name
  data_root: # the path to your data
  train_list: # your train.txt
  val_list: # your val.txt
  classes: 2

  
TRAIN:
  arch: ours
  layers: 50
  sync_bn: False
  train_h: 200
  train_w: 200
  val_size: 200
  scale_min: 0.9    # minimum random scale
  scale_max: 1.1    # maximum random scale
  rotate_min: -10   # minimum random rotate
  rotate_max: 10    # maximum random rotate
  zoom_factor: 8
  ignore_label: 255
  padding_label: 255
  aux_weight: 1.0
  train_gpu: [0]
  workers: 4        # data loader workers 2->4
  batch_size: 2     # batch size for training
  batch_size_val: 10
  base_lr: 0.005   # batch size 2 -> 0.001
  epochs: 200
  start_epoch: 0
  power: 0.9        # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed: 1998
  print_freq: 5
  save_freq: 5

  save_path: final/fold0  # model saved path
  shot: 1
  weight: 
     # load weight for fine-tuning or testing
  resume:                                         # path to latest checkpoint (default: none)
  evaluate: True
  split: 0
  vgg: False # use vgg as backbone or not

  ppm_scales: [60, 30, 15, 8]
  fix_random_seed_val: True
  warmup: False
  resized_val: True
  ori_resize: True  # use original label for evaluation
  use_coco: False
  use_split_coco: False
  output_mask_dir: mask/fold0  #the path to save the mask images


## deprecated multi-processing training
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0
  use_apex: False
  opt_level: 'O0'
  keep_batchnorm_fp32:
  loss_scale: